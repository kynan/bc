{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: Qt4Agg\n",
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is an optimization algorithm to find the local minimum of some cost function. It is a popular method for computing linear regressions. We want to use it to fit a straight line through a number of measurement points.\n",
      "\n",
      "The cost function to minimize is given by\n",
      "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
      "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
      "$$h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1$$\n",
      "\n",
      "The objective is therefore to find the $\\theta$ that minimizes $J(\\theta)$. In the batch gradient descent algorithm, the current guess $\\theta_j$ is updated in each iteration by taking a weighted step in the direction of the gradient (the weight $\\alpha$ is often called *learning rate*):\n",
      "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j} J(\\theta_0, \\theta_1)$$\n",
      "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
      "\n",
      "Note that in practice, the vector $x$ is prepended with a vector of ones, such that $h$ can be computed as\n",
      "$$h_\\theta(x) = \\theta^T x = \\theta_0 x_0 + \\theta_1 x_1$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A naive implementation of the cost function is as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def J(theta, X, y):\n",
      "    m = len(y)\n",
      "    cost = 0\n",
      "    for i in range(m):\n",
      "        h = theta[0]*X[i,0] + theta[1]*X[i,1]\n",
      "        cost += (h - y[i])**2\n",
      "    return cost/(2*m)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Similarly, we can implement gradient descent like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_descent(X, y, theta, alpha, num_it):\n",
      "    m = len(y)\n",
      "    Js = zeros(num_it)\n",
      "    def grad(n):\n",
      "        s = 0\n",
      "        for i in range(m):\n",
      "            h = theta[0]*X[i,0] + theta[1]*X[i,1]\n",
      "            s += (h - y[i])*X[i,n]\n",
      "        return s\n",
      "    for it in range(num_it):\n",
      "        theta[0] -= grad(0) * alpha/m\n",
      "        theta[1] -= grad(1) * alpha/m\n",
      "        Js[it] = J(theta, X, y)\n",
      "    return Js, theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let us evaluate the performance of this implementation on a random input of length 10000, obtained as a pertibuation of values on the straight line $0.4 x + 2$, which means we know our expected $\\theta$ will be close to `[2, 0.4]`. In practice, `x` and `y` are data obtained from experiments."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = 10000\n",
      "X = np.ones((N, 2))\n",
      "X[:,1] = np.random.random(N)\n",
      "a = 0.4\n",
      "b = 2\n",
      "y = a*X[:,1] + b + np.random.random(N) - 0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To measure the runtime of the gradient descent for 100 iterations, we use the IPython `%time` magic, which measures CPU and wallclock time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time Js, theta = gradient_descent(X, y, [0, 0], 0.1, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 9.13 s, sys: 36 ms, total: 9.17 s\n",
        "Wall time: 8.54 s\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get a more detailed breakdown of where the time was spent, we can get a profile of the execution with the IPython `%prun` magic, which reveals how much time was spent in each particular function call:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%prun -s cumtime gradient_descent(X, y, [0, 0], 0.1, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tells us that the majority of the time was spent in the `grad` function to compute the gradient for the $\\theta$ update and another significant portion to compute the cost function `J`.\n",
      "\n",
      "Before we try to optimize our code, let us confirm we found a good approximation to our data. We expect `theta` to be somewhat close to `[2, 0.4]`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[1.8806083968834593, 0.62088278092134108]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To study the convergence behaviour we plot the cost function which we have computed after each iteration:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(Js)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[<matplotlib.lines.Line2D at 0x3c758d0>]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Already after roughly 20 iterations the cost no longer decreases substantially, which indicates that the algorithm has converged.\n",
      "\n",
      "We can overlay the linear regression over the input data to convince ourselves that the line is a good fit to the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figure()\n",
      "plot(X[:,1], y, '+')\n",
      "x = np.linspace(0, 1, 101)\n",
      "plot(x, theta[1]*x + theta[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[<matplotlib.lines.Line2D at 0x3f86bd0>]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Optimization\n",
      "\n",
      "Looking closely at the definition of the cost function, we realise it can be expressed in vectorized form:\n",
      "$$J(\\theta) = \\frac{1}{2m} (X\\theta - \\vec{y})^T (X\\theta - \\vec{y})$$\n",
      "\n",
      "We can significantly improve the performance of our naive implementation by vectorization: we replace the (slow) loops by fast NumPy operations on vectors. For the computation of the cost function we can express both $h$ and $J$ as inner products, computed with NumPy's `dot` function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def J(theta, X, y):\n",
      "    m = len(y)\n",
      "    hy = np.dot(X, theta) - y\n",
      "    return np.dot(hy, hy) / (2*m)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The same is true for the computation of the gradient, which we can again express as an inner product. This allows us to define a single update step for all components of $\\theta$:\n",
      "$$\\theta := \\theta - \\frac{\\alpha}{m} X^T (X\\theta - \\vec{y})$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_descent(X, y, theta, alpha, num_it):\n",
      "    m = len(y)\n",
      "    Js = zeros(num_it)\n",
      "    for it in range(num_it):\n",
      "        h = np.dot(X, theta)\n",
      "        theta = theta - alpha * np.dot(X.T, (h - y)) / m\n",
      "        Js[it] = J(theta, X, y)\n",
      "    return Js, theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Timing the execution of this optimized function reveals a significat speedup of more that a factor of 100! Not only is the vectorized implementation faster, it is also much more compact and easier to read. Furthermore, it extends naturally to the case of linear regression with multiple variables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time Js, theta = gradient_descent(X, y, np.array([0, 0]).T, 0.1, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 44 ms, sys: 0 ns, total: 44 ms\n",
        "Wall time: 46.3 ms\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we convince ourselves that we get approximately the same result as the naive version:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "theta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "array([ 1.85893031,  0.66168868])"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looking at a profile of this computation reveals that we now spend most of our time computing inner products:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%prun -s cumtime Js, theta = gradient_descent(X, y, np.array([0, 0]).T, 0.1, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}